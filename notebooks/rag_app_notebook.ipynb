{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbeYKU6f8L4w"
      },
      "source": [
        "# RAG Application with Gemini API and LangChain\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
        "- **LLM**: Google Gemini 2.5 Flash (via Google AI Studio - Free Tier)\n",
        "- **Embeddings**: Gemini Embedding model (gemini-embedding-001)\n",
        "- **Vector Store**: FAISS (in-memory)\n",
        "- **Framework**: LangChain with modern LCEL patterns (1.0 compatible)\n",
        "\n",
        "## Features\n",
        "- Upload and process PDF, TXT, and DOCX files\n",
        "- Create embeddings and store in vector database\n",
        "- Ask questions and get answers with source citations\n",
        "- All in-memory (no persistence between sessions)\n",
        "- Free tier usage only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pBo0nk78L4w"
      },
      "source": [
        "## 1. Installation and Setup\n",
        "\n",
        "First, let's install all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zEPUvMGG8L4w"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q -U \\\n",
        "    langchain \\\n",
        "    langchain-classic \\\n",
        "    langchain-google-genai \\\n",
        "    langchain-community \\\n",
        "    langchain-text-splitters \\\n",
        "    faiss-cpu \\\n",
        "    pypdf \\\n",
        "    python-docx \\\n",
        "    python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNt2-M_68L4x"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mrf_3E--8L4x",
        "outputId": "20571ca2-a49a-413e-f37e-9a8e9bd82c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# LangChain core imports\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Document loaders\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    TextLoader,\n",
        "    Docx2txtLoader\n",
        ")\n",
        "\n",
        "# Vector store\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Google Gemini imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Modern LangChain chain constructors (LCEL)\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdi7vEFX8L4y"
      },
      "source": [
        "## 3. Configuration\n",
        "\n",
        "### Get Your Free Gemini API Key:\n",
        "1. Go to [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
        "2. Click \"Create API Key\"\n",
        "3. Copy the key and paste it below\n",
        "\n",
        "**Note**: The free tier provides:\n",
        "- 1,500 requests per day\n",
        "- 15 requests per minute\n",
        "- 1 million tokens per minute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrvpZyCW8L4y",
        "outputId": "5b1edee3-f1d3-4c43-fbfa-92112840072e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ API key configured!\n"
          ]
        }
      ],
      "source": [
        "# Set your Gemini API key\n",
        "# Option 1: Direct input\n",
        "GOOGLE_API_KEY = \"GOOGLE_API_KEY\"  \n",
        "\n",
        "# Option 2: Use environment variable\n",
        "# GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "print(\"✓ API key configured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFsZANOq8L4y"
      },
      "source": [
        "### Initialize Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnrF5Qh88L4y",
        "outputId": "0c6b594b-8c8a-4335-8861-703d7af98d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LLM and Embeddings models initialized!\n"
          ]
        }
      ],
      "source": [
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",  # Free tier model\n",
        "    temperature=0,  # For more deterministic outputs\n",
        "    max_tokens=1024\n",
        ")\n",
        "\n",
        "# Initialize Gemini Embeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"gemini-embedding-001\"  # Gemini embedding model\n",
        ")\n",
        "\n",
        "print(\"✓ LLM and Embeddings models initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3gQlw4M8L4z"
      },
      "source": [
        "### Test the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOUv0R1W8L4z",
        "outputId": "88b8f3e5-82c1-47dd-f4bd-7524ebe2d3d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Response: Hello World!\n",
            "✓ LLM is working!\n",
            "\n",
            "Embedding dimension: 3072\n",
            "First 5 values: [-0.020297376438975334, 0.0038267294876277447, 0.016992559656500816, -0.09309638291597366, -0.0009401048882864416]\n",
            "✓ Embeddings are working!\n"
          ]
        }
      ],
      "source": [
        "# Test LLM\n",
        "try:\n",
        "    response = llm.invoke(\"Say 'Hello World' if you're working!\")\n",
        "    print(\"LLM Response:\", response.content)\n",
        "    print(\"✓ LLM is working!\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ LLM Error: {e}\\n\")\n",
        "\n",
        "# Test Embeddings\n",
        "try:\n",
        "    test_embedding = embeddings.embed_query(\"test\")\n",
        "    print(f\"Embedding dimension: {len(test_embedding)}\")\n",
        "    print(f\"First 5 values: {test_embedding[:5]}\")\n",
        "    print(\"✓ Embeddings are working!\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Embeddings Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALZMgK278L4z"
      },
      "source": [
        "## 4. Document Loading Functions\n",
        "\n",
        "Functions to load different document types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uGGFXXt8L40",
        "outputId": "06548c80-7930-4235-a3a0-aa8eb8f10208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Document loading functions defined!\n"
          ]
        }
      ],
      "source": [
        "def load_document(file_path: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load a document based on its file extension.\n",
        "\n",
        "    Supported formats: .pdf, .txt, .docx\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the document file\n",
        "\n",
        "    Returns:\n",
        "        List of Document objects\n",
        "    \"\"\"\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    try:\n",
        "        if file_extension == \".pdf\":\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file_extension == \".txt\":\n",
        "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        elif file_extension == \".docx\":\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
        "\n",
        "        documents = loader.load()\n",
        "        print(f\"✓ Loaded {len(documents)} document(s) from {file_path}\")\n",
        "        return documents\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading document: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def load_document_from_text(text: str, metadata: Dict[str, Any] = None) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Create a document from raw text.\n",
        "\n",
        "    Args:\n",
        "        text: Raw text content\n",
        "        metadata: Optional metadata dictionary\n",
        "\n",
        "    Returns:\n",
        "        List containing a single Document object\n",
        "    \"\"\"\n",
        "    if metadata is None:\n",
        "        metadata = {\"source\": \"user_input\"}\n",
        "\n",
        "    doc = Document(page_content=text, metadata=metadata)\n",
        "    print(f\"✓ Created document from text ({len(text)} characters)\")\n",
        "    return [doc]\n",
        "\n",
        "\n",
        "print(\"✓ Document loading functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s95WzrzU8L40"
      },
      "source": [
        "## 5. Text Splitting\n",
        "\n",
        "Split documents into smaller chunks for better retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs7yd4jl8L40",
        "outputId": "5c501ab2-21bd-4d14-d522-0a5edda35b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Text splitting function defined!\n"
          ]
        }
      ],
      "source": [
        "def split_documents(documents: List[Document],\n",
        "                   chunk_size: int = 1000,\n",
        "                   chunk_overlap: int = 200) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Split documents into smaller chunks.\n",
        "\n",
        "    Args:\n",
        "        documents: List of Document objects to split\n",
        "        chunk_size: Maximum size of each chunk in characters\n",
        "        chunk_overlap: Number of characters to overlap between chunks\n",
        "\n",
        "    Returns:\n",
        "        List of split Document objects\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    print(f\"✓ Split into {len(splits)} chunks\")\n",
        "\n",
        "    # Show first chunk as example\n",
        "    if splits:\n",
        "        print(f\"\\nExample chunk (first 200 chars):\")\n",
        "        print(f\"{splits[0].page_content[:200]}...\\n\")\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "print(\"✓ Text splitting function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RqzYMbw8L40"
      },
      "source": [
        "## 6. Vector Store Creation\n",
        "\n",
        "Create a FAISS vector store from document chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsvtyVpX8L40",
        "outputId": "94325e89-0709-4d8d-d56f-905e26ca2548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Vector store function defined!\n"
          ]
        }
      ],
      "source": [
        "def create_vector_store(chunks: List[Document],\n",
        "                       embeddings_model: GoogleGenerativeAIEmbeddings) -> FAISS:\n",
        "    \"\"\"\n",
        "    Create a FAISS vector store from document chunks.\n",
        "\n",
        "    Args:\n",
        "        chunks: List of document chunks\n",
        "        embeddings_model: Embeddings model to use\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store\n",
        "    \"\"\"\n",
        "    print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
        "    print(\"(This may take a moment with free tier rate limits)\\n\")\n",
        "\n",
        "    try:\n",
        "        vectorstore = FAISS.from_documents(\n",
        "            documents=chunks,\n",
        "            embedding=embeddings_model\n",
        "        )\n",
        "        print(\"✓ Vector store created successfully!\")\n",
        "        return vectorstore\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating vector store: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "print(\"✓ Vector store function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIvpMp3k8L41"
      },
      "source": [
        "## 7. RAG Chain Creation (Modern LCEL Pattern)\n",
        "\n",
        "This uses the modern `create_retrieval_chain` pattern that's compatible with LangChain 1.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8HFZtuB8L41",
        "outputId": "b782990a-8daa-43fa-ab24-d7a6938be08a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ RAG chain function defined!\n"
          ]
        }
      ],
      "source": [
        "def create_rag_chain(vectorstore: FAISS,\n",
        "                     llm_model: ChatGoogleGenerativeAI,\n",
        "                     k: int = 4):\n",
        "    \"\"\"\n",
        "    Create a RAG chain using modern LCEL patterns.\n",
        "\n",
        "    Args:\n",
        "        vectorstore: FAISS vector store\n",
        "        llm_model: Language model to use\n",
        "        k: Number of documents to retrieve\n",
        "\n",
        "    Returns:\n",
        "        RAG chain (Runnable)\n",
        "    \"\"\"\n",
        "    # Create retriever from vector store\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": k}\n",
        "    )\n",
        "\n",
        "    # Define the prompt template\n",
        "    system_prompt = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of retrieved context to answer the question. \"\n",
        "        \"If you don't know the answer, just say that you don't know. \"\n",
        "        \"Use three sentences maximum and keep the answer concise.\\n\\n\"\n",
        "        \"Context: {context}\"\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create the document chain (combines documents and generates answer)\n",
        "    question_answer_chain = create_stuff_documents_chain(llm_model, prompt)\n",
        "\n",
        "    # Create the full RAG chain (retrieval + generation)\n",
        "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "    print(\"✓ RAG chain created successfully!\")\n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "print(\"✓ RAG chain function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSO2YRLa8L41"
      },
      "source": [
        "## 8. Query Function with Source Display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-06c4Mva8L41",
        "outputId": "df2d9648-f843-4af2-ad88-161223e575df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Query function defined!\n"
          ]
        }
      ],
      "source": [
        "def query_rag(chain, question: str, show_sources: bool = True) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Query the RAG chain and display results.\n",
        "\n",
        "    Args:\n",
        "        chain: The RAG chain\n",
        "        question: Question to ask\n",
        "        show_sources: Whether to display source documents\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with answer and context\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"QUESTION: {question}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "\n",
        "    try:\n",
        "        # Invoke the chain\n",
        "        result = chain.invoke({\"input\": question})\n",
        "\n",
        "        # Display answer\n",
        "        print(\"ANSWER:\")\n",
        "        print(result[\"answer\"])\n",
        "        print()\n",
        "\n",
        "        # Display sources if requested\n",
        "        if show_sources and \"context\" in result:\n",
        "            print(f\"\\n{'─'*70}\")\n",
        "            print(f\"SOURCES ({len(result['context'])} documents retrieved):\")\n",
        "            print(f\"{'─'*70}\\n\")\n",
        "\n",
        "            for i, doc in enumerate(result[\"context\"], 1):\n",
        "                print(f\"Source {i}:\")\n",
        "                print(f\"Content: {doc.page_content[:300]}...\")\n",
        "                if doc.metadata:\n",
        "                    print(f\"Metadata: {doc.metadata}\")\n",
        "                print()\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error querying RAG chain: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "print(\"✓ Query function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2H7NntT8L41"
      },
      "source": [
        "## 9. Complete RAG Pipeline Class\n",
        "\n",
        "A wrapper class to manage the entire RAG workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2ZNN2le8L41",
        "outputId": "70648357-4996-4280-f169-184683f8f573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ RAGSystem class defined!\n"
          ]
        }
      ],
      "source": [
        "class RAGSystem:\n",
        "    \"\"\"\n",
        "    Complete RAG system that handles document loading, processing, and querying.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm, embeddings, chunk_size=1000, chunk_overlap=200, k=4):\n",
        "        \"\"\"\n",
        "        Initialize the RAG system.\n",
        "\n",
        "        Args:\n",
        "            llm: Language model\n",
        "            embeddings: Embeddings model\n",
        "            chunk_size: Size of text chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "            k: Number of documents to retrieve\n",
        "        \"\"\"\n",
        "        self.llm = llm\n",
        "        self.embeddings = embeddings\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.k = k\n",
        "\n",
        "        self.documents = []\n",
        "        self.chunks = []\n",
        "        self.vectorstore = None\n",
        "        self.rag_chain = None\n",
        "\n",
        "    def load_from_file(self, file_path: str):\n",
        "        \"\"\"Load document from file.\"\"\"\n",
        "        self.documents = load_document(file_path)\n",
        "        return self\n",
        "\n",
        "    def load_from_text(self, text: str, metadata: Dict[str, Any] = None):\n",
        "        \"\"\"Load document from text.\"\"\"\n",
        "        self.documents = load_document_from_text(text, metadata)\n",
        "        return self\n",
        "\n",
        "    def process_documents(self):\n",
        "        \"\"\"Split documents and create vector store.\"\"\"\n",
        "        if not self.documents:\n",
        "            print(\"❌ No documents loaded!\")\n",
        "            return self\n",
        "\n",
        "        # Split documents\n",
        "        self.chunks = split_documents(\n",
        "            self.documents,\n",
        "            self.chunk_size,\n",
        "            self.chunk_overlap\n",
        "        )\n",
        "\n",
        "        # Create vector store\n",
        "        self.vectorstore = create_vector_store(self.chunks, self.embeddings)\n",
        "\n",
        "        # Create RAG chain\n",
        "        self.rag_chain = create_rag_chain(self.vectorstore, self.llm, self.k)\n",
        "\n",
        "        print(\"\\n✓ RAG system ready for queries!\\n\")\n",
        "        return self\n",
        "\n",
        "    def query(self, question: str, show_sources: bool = True):\n",
        "        \"\"\"Query the RAG system.\"\"\"\n",
        "        if self.rag_chain is None:\n",
        "            print(\"❌ RAG chain not initialized. Call process_documents() first!\")\n",
        "            return {}\n",
        "\n",
        "        return query_rag(self.rag_chain, question, show_sources)\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get statistics about the loaded documents.\"\"\"\n",
        "        stats = {\n",
        "            \"num_documents\": len(self.documents),\n",
        "            \"num_chunks\": len(self.chunks),\n",
        "            \"chunk_size\": self.chunk_size,\n",
        "            \"chunk_overlap\": self.chunk_overlap,\n",
        "            \"retrieval_k\": self.k\n",
        "        }\n",
        "\n",
        "        print(\"\\nRAG System Statistics:\")\n",
        "        print(\"─\" * 40)\n",
        "        for key, value in stats.items():\n",
        "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "        print(\"─\" * 40 + \"\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "\n",
        "print(\"✓ RAGSystem class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jewM9Uka8L42"
      },
      "source": [
        "## 10. Example Usage\n",
        "\n",
        "Let's test the system with sample text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bOS-tEn8L42"
      },
      "source": [
        "### Example 1: Using Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs_q9ur08L42",
        "outputId": "b1c5dd28-6c22-4ea1-efc8-3a75985d5649"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Created document from text (2123 characters)\n",
            "✓ Split into 6 chunks\n",
            "\n",
            "Example chunk (first 200 chars):\n",
            "LangChain is a framework for developing applications powered by language models.\n",
            "It enables applications that are context-aware and can reason about how to answer\n",
            "based on provided context. The framew...\n",
            "\n",
            "Creating embeddings for 6 chunks...\n",
            "(This may take a moment with free tier rate limits)\n",
            "\n",
            "✓ Vector store created successfully!\n",
            "✓ RAG chain created successfully!\n",
            "\n",
            "✓ RAG system ready for queries!\n",
            "\n",
            "\n",
            "RAG System Statistics:\n",
            "────────────────────────────────────────\n",
            "Num Documents: 1\n",
            "Num Chunks: 6\n",
            "Chunk Size: 500\n",
            "Chunk Overlap: 100\n",
            "Retrieval K: 3\n",
            "────────────────────────────────────────\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'num_documents': 1,\n",
              " 'num_chunks': 6,\n",
              " 'chunk_size': 500,\n",
              " 'chunk_overlap': 100,\n",
              " 'retrieval_k': 3}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample document about AI and LangChain\n",
        "sample_text = \"\"\"\n",
        "LangChain is a framework for developing applications powered by language models.\n",
        "It enables applications that are context-aware and can reason about how to answer\n",
        "based on provided context. The framework consists of several key components:\n",
        "\n",
        "Models: LangChain provides abstractions for working with different language models\n",
        "from various providers like OpenAI, Anthropic, and Google. These models can be\n",
        "easily swapped without changing the application code.\n",
        "\n",
        "Prompts: The framework includes tools for managing and optimizing prompts, which\n",
        "are the inputs given to language models. Prompt templates allow for dynamic\n",
        "generation of prompts based on user input.\n",
        "\n",
        "Memory: LangChain provides different types of memory components that allow\n",
        "applications to maintain context across multiple interactions. This is essential\n",
        "for building chatbots and conversational AI systems.\n",
        "\n",
        "Chains: These are sequences of calls to LLMs or other tools. Chains can be simple\n",
        "(calling a single LLM) or complex (calling multiple LLMs or tools in sequence).\n",
        "The newest approach uses LangChain Expression Language (LCEL) which provides\n",
        "better composability and streaming support.\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\n",
        "LangChain. RAG combines the power of language models with external knowledge bases.\n",
        "It works by first retrieving relevant documents from a knowledge base, then using\n",
        "those documents as context for the language model to generate an answer.\n",
        "\n",
        "Vector Stores are a key component in RAG systems. They store document embeddings,\n",
        "which are numerical representations of text that capture semantic meaning. When a\n",
        "user asks a question, the question is also embedded, and the vector store finds\n",
        "the most similar document embeddings, effectively finding the most relevant documents.\n",
        "\n",
        "The Gemini API from Google provides both powerful language models and embedding\n",
        "models. Gemini 2.0 Flash is optimized for speed and efficiency, while maintaining\n",
        "high quality outputs. The Gemini Embedding model produces state-of-the-art embeddings\n",
        "that work across multiple languages and understand nuanced context.\n",
        "\"\"\"\n",
        "\n",
        "# Initialize RAG system\n",
        "rag = RAGSystem(\n",
        "    llm=llm,\n",
        "    embeddings=embeddings,\n",
        "    chunk_size=500,  # Smaller chunks for this example\n",
        "    chunk_overlap=100,\n",
        "    k=3  # Retrieve top 3 chunks\n",
        ")\n",
        "\n",
        "# Load and process the document\n",
        "rag.load_from_text(sample_text, metadata={\"source\": \"langchain_intro\"})\n",
        "rag.process_documents()\n",
        "\n",
        "# Get statistics\n",
        "rag.get_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5UWytb8L42"
      },
      "source": [
        "### Ask Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmq06t-R8L42",
        "outputId": "31f89e19-b006-4b7a-a52f-130bfe522688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "QUESTION: What is LangChain?\n",
            "======================================================================\n",
            "\n",
            "ANSWER:\n",
            "LangChain is a framework designed for developing applications powered by language models. It enables applications to be context-aware and reason about how to answer based on provided context. The framework includes components like models, prompts, memory, and chains to facilitate this development.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "SOURCES (3 documents retrieved):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Source 1:\n",
            "Content: LangChain is a framework for developing applications powered by language models.\n",
            "It enables applications that are context-aware and can reason about how to answer\n",
            "based on provided context. The framework consists of several key components:\n",
            "\n",
            "Models: LangChain provides abstractions for working with di...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 2:\n",
            "Content: Prompts: The framework includes tools for managing and optimizing prompts, which\n",
            "are the inputs given to language models. Prompt templates allow for dynamic\n",
            "generation of prompts based on user input.\n",
            "\n",
            "Memory: LangChain provides different types of memory components that allow\n",
            "applications to maintain...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 3:\n",
            "Content: Chains: These are sequences of calls to LLMs or other tools. Chains can be simple\n",
            "(calling a single LLM) or complex (calling multiple LLMs or tools in sequence).\n",
            "The newest approach uses LangChain Expression Language (LCEL) which provides\n",
            "better composability and streaming support....\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is LangChain?',\n",
              " 'context': [Document(id='8bed6b5f-0590-4f08-8d01-0b6d885237bc', metadata={'source': 'langchain_intro'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables applications that are context-aware and can reason about how to answer\\nbased on provided context. The framework consists of several key components:\\n\\nModels: LangChain provides abstractions for working with different language models\\nfrom various providers like OpenAI, Anthropic, and Google. These models can be\\neasily swapped without changing the application code.'),\n",
              "  Document(id='b2191d88-148e-44fb-b7f7-8af0d77281b2', metadata={'source': 'langchain_intro'}, page_content='Prompts: The framework includes tools for managing and optimizing prompts, which\\nare the inputs given to language models. Prompt templates allow for dynamic\\ngeneration of prompts based on user input.\\n\\nMemory: LangChain provides different types of memory components that allow\\napplications to maintain context across multiple interactions. This is essential\\nfor building chatbots and conversational AI systems.'),\n",
              "  Document(id='e2eb3498-598c-4657-8f2e-e2e7ad0eb2d3', metadata={'source': 'langchain_intro'}, page_content='Chains: These are sequences of calls to LLMs or other tools. Chains can be simple\\n(calling a single LLM) or complex (calling multiple LLMs or tools in sequence).\\nThe newest approach uses LangChain Expression Language (LCEL) which provides\\nbetter composability and streaming support.')],\n",
              " 'answer': 'LangChain is a framework designed for developing applications powered by language models. It enables applications to be context-aware and reason about how to answer based on provided context. The framework includes components like models, prompts, memory, and chains to facilitate this development.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 1\n",
        "rag.query(\"What is LangChain?\", show_sources=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02ogRBTy8L42",
        "outputId": "23b25140-50af-4317-9d72-338e1f721acf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "QUESTION: How does RAG work?\n",
            "======================================================================\n",
            "\n",
            "ANSWER:\n",
            "RAG works by first retrieving relevant documents from an external knowledge base. These retrieved documents are then used as context for a language model. The language model then generates an answer based on this provided context.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "SOURCES (3 documents retrieved):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Source 1:\n",
            "Content: Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\n",
            "LangChain. RAG combines the power of language models with external knowledge bases.\n",
            "It works by first retrieving relevant documents from a knowledge base, then using\n",
            "those documents as context for the language model to ...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 2:\n",
            "Content: Vector Stores are a key component in RAG systems. They store document embeddings,\n",
            "which are numerical representations of text that capture semantic meaning. When a\n",
            "user asks a question, the question is also embedded, and the vector store finds\n",
            "the most similar document embeddings, effectively findin...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 3:\n",
            "Content: LangChain is a framework for developing applications powered by language models.\n",
            "It enables applications that are context-aware and can reason about how to answer\n",
            "based on provided context. The framework consists of several key components:\n",
            "\n",
            "Models: LangChain provides abstractions for working with di...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'How does RAG work?',\n",
              " 'context': [Document(id='a3dad085-2457-4022-86de-4a249fc41965', metadata={'source': 'langchain_intro'}, page_content='Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\\nLangChain. RAG combines the power of language models with external knowledge bases.\\nIt works by first retrieving relevant documents from a knowledge base, then using\\nthose documents as context for the language model to generate an answer.'),\n",
              "  Document(id='b20aeffd-e951-4175-aee0-1d30537119bb', metadata={'source': 'langchain_intro'}, page_content='Vector Stores are a key component in RAG systems. They store document embeddings,\\nwhich are numerical representations of text that capture semantic meaning. When a\\nuser asks a question, the question is also embedded, and the vector store finds\\nthe most similar document embeddings, effectively finding the most relevant documents.'),\n",
              "  Document(id='8bed6b5f-0590-4f08-8d01-0b6d885237bc', metadata={'source': 'langchain_intro'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables applications that are context-aware and can reason about how to answer\\nbased on provided context. The framework consists of several key components:\\n\\nModels: LangChain provides abstractions for working with different language models\\nfrom various providers like OpenAI, Anthropic, and Google. These models can be\\neasily swapped without changing the application code.')],\n",
              " 'answer': 'RAG works by first retrieving relevant documents from an external knowledge base. These retrieved documents are then used as context for a language model. The language model then generates an answer based on this provided context.'}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 2\n",
        "rag.query(\"How does RAG work?\", show_sources=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tghaxtCP8L42",
        "outputId": "89af8c33-5987-448a-8525-8bb23cc0dd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "QUESTION: What are vector stores used for?\n",
            "======================================================================\n",
            "\n",
            "ANSWER:\n",
            "Vector stores are used to store document embeddings, which are numerical representations of text capturing semantic meaning. In RAG systems, they help find the most relevant documents by comparing the embedding of a user's question to stored document embeddings. This process effectively retrieves documents most similar to the query.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "SOURCES (3 documents retrieved):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Source 1:\n",
            "Content: Vector Stores are a key component in RAG systems. They store document embeddings,\n",
            "which are numerical representations of text that capture semantic meaning. When a\n",
            "user asks a question, the question is also embedded, and the vector store finds\n",
            "the most similar document embeddings, effectively findin...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 2:\n",
            "Content: Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\n",
            "LangChain. RAG combines the power of language models with external knowledge bases.\n",
            "It works by first retrieving relevant documents from a knowledge base, then using\n",
            "those documents as context for the language model to ...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 3:\n",
            "Content: LangChain is a framework for developing applications powered by language models.\n",
            "It enables applications that are context-aware and can reason about how to answer\n",
            "based on provided context. The framework consists of several key components:\n",
            "\n",
            "Models: LangChain provides abstractions for working with di...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What are vector stores used for?',\n",
              " 'context': [Document(id='b20aeffd-e951-4175-aee0-1d30537119bb', metadata={'source': 'langchain_intro'}, page_content='Vector Stores are a key component in RAG systems. They store document embeddings,\\nwhich are numerical representations of text that capture semantic meaning. When a\\nuser asks a question, the question is also embedded, and the vector store finds\\nthe most similar document embeddings, effectively finding the most relevant documents.'),\n",
              "  Document(id='a3dad085-2457-4022-86de-4a249fc41965', metadata={'source': 'langchain_intro'}, page_content='Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\\nLangChain. RAG combines the power of language models with external knowledge bases.\\nIt works by first retrieving relevant documents from a knowledge base, then using\\nthose documents as context for the language model to generate an answer.'),\n",
              "  Document(id='8bed6b5f-0590-4f08-8d01-0b6d885237bc', metadata={'source': 'langchain_intro'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables applications that are context-aware and can reason about how to answer\\nbased on provided context. The framework consists of several key components:\\n\\nModels: LangChain provides abstractions for working with different language models\\nfrom various providers like OpenAI, Anthropic, and Google. These models can be\\neasily swapped without changing the application code.')],\n",
              " 'answer': \"Vector stores are used to store document embeddings, which are numerical representations of text capturing semantic meaning. In RAG systems, they help find the most relevant documents by comparing the embedding of a user's question to stored document embeddings. This process effectively retrieves documents most similar to the query.\"}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 3\n",
        "rag.query(\"What are vector stores used for?\", show_sources=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrvW9C9H8L42",
        "outputId": "6a204cdc-3a73-4cf2-8804-48e3e149f05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "QUESTION: What is the capital of France?\n",
            "======================================================================\n",
            "\n",
            "ANSWER:\n",
            "I don't know the answer to that question based on the provided context. The context describes LangChain and its components, not geographical facts.\n",
            "\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "SOURCES (3 documents retrieved):\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "\n",
            "Source 1:\n",
            "Content: LangChain is a framework for developing applications powered by language models.\n",
            "It enables applications that are context-aware and can reason about how to answer\n",
            "based on provided context. The framework consists of several key components:\n",
            "\n",
            "Models: LangChain provides abstractions for working with di...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 2:\n",
            "Content: Prompts: The framework includes tools for managing and optimizing prompts, which\n",
            "are the inputs given to language models. Prompt templates allow for dynamic\n",
            "generation of prompts based on user input.\n",
            "\n",
            "Memory: LangChain provides different types of memory components that allow\n",
            "applications to maintain...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n",
            "Source 3:\n",
            "Content: Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\n",
            "LangChain. RAG combines the power of language models with external knowledge bases.\n",
            "It works by first retrieving relevant documents from a knowledge base, then using\n",
            "those documents as context for the language model to ...\n",
            "Metadata: {'source': 'langchain_intro'}\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the capital of France?',\n",
              " 'context': [Document(id='8bed6b5f-0590-4f08-8d01-0b6d885237bc', metadata={'source': 'langchain_intro'}, page_content='LangChain is a framework for developing applications powered by language models.\\nIt enables applications that are context-aware and can reason about how to answer\\nbased on provided context. The framework consists of several key components:\\n\\nModels: LangChain provides abstractions for working with different language models\\nfrom various providers like OpenAI, Anthropic, and Google. These models can be\\neasily swapped without changing the application code.'),\n",
              "  Document(id='b2191d88-148e-44fb-b7f7-8af0d77281b2', metadata={'source': 'langchain_intro'}, page_content='Prompts: The framework includes tools for managing and optimizing prompts, which\\nare the inputs given to language models. Prompt templates allow for dynamic\\ngeneration of prompts based on user input.\\n\\nMemory: LangChain provides different types of memory components that allow\\napplications to maintain context across multiple interactions. This is essential\\nfor building chatbots and conversational AI systems.'),\n",
              "  Document(id='a3dad085-2457-4022-86de-4a249fc41965', metadata={'source': 'langchain_intro'}, page_content='Retrieval-Augmented Generation (RAG) is one of the most powerful applications of\\nLangChain. RAG combines the power of language models with external knowledge bases.\\nIt works by first retrieving relevant documents from a knowledge base, then using\\nthose documents as context for the language model to generate an answer.')],\n",
              " 'answer': \"I don't know the answer to that question based on the provided context. The context describes LangChain and its components, not geographical facts.\"}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Question 4 - Testing out of context\n",
        "rag.query(\"What is the capital of France?\", show_sources=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZ1J3rZ8L43"
      },
      "source": [
        "### Example 2: Loading from a File\n",
        "\n",
        "Uncomment and modify the path to load from an actual file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT-Cuk6y8L43"
      },
      "outputs": [],
      "source": [
        "# # Create a new RAG system for file-based document\n",
        "# rag_file = RAGSystem(\n",
        "#     llm=llm,\n",
        "#     embeddings=embeddings,\n",
        "#     chunk_size=1000,\n",
        "#     chunk_overlap=200,\n",
        "#     k=4\n",
        "# )\n",
        "\n",
        "# # Load from file (replace with your file path)\n",
        "# file_path = \"path/to/your/document.pdf\"  # or .txt, .docx\n",
        "# rag_file.load_from_file(file_path)\n",
        "# rag_file.process_documents()\n",
        "\n",
        "# # Query the document\n",
        "# rag_file.query(\"Your question here\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_iFW3nC8L43"
      },
      "source": [
        "## 11. Advanced Features\n",
        "\n",
        "### Direct Vector Store Search (without LLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rntY_m4s8L43"
      },
      "outputs": [],
      "source": [
        "def search_similar_documents(vectorstore: FAISS, query: str, k: int = 3):\n",
        "    \"\"\"\n",
        "    Search for similar documents without generating an answer.\n",
        "    Useful for understanding what context is being retrieved.\n",
        "    \"\"\"\n",
        "    print(f\"\\nSearching for documents similar to: '{query}'\\n\")\n",
        "    print(\"─\" * 70)\n",
        "\n",
        "    # Perform similarity search\n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"\\nDocument {i}:\")\n",
        "        print(f\"Content: {doc.page_content[:400]}...\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print(\"─\" * 70)\n",
        "\n",
        "# Try it out\n",
        "search_similar_documents(rag.vectorstore, \"embeddings and vector stores\", k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-YTZODm8L43"
      },
      "source": [
        "### Similarity Search with Scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZoHTOEg8L43"
      },
      "outputs": [],
      "source": [
        "def search_with_scores(vectorstore: FAISS, query: str, k: int = 3):\n",
        "    \"\"\"\n",
        "    Search with similarity scores to see how relevant the documents are.\n",
        "    Lower scores mean more similar.\n",
        "    \"\"\"\n",
        "    print(f\"\\nSearching with scores for: '{query}'\\n\")\n",
        "    print(\"─\" * 70)\n",
        "\n",
        "    docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    for i, (doc, score) in enumerate(docs_with_scores, 1):\n",
        "        print(f\"\\nDocument {i} (Score: {score:.4f}):\")\n",
        "        print(f\"Content: {doc.page_content[:300]}...\")\n",
        "        print(\"─\" * 70)\n",
        "\n",
        "# Try it out\n",
        "search_with_scores(rag.vectorstore, \"What is a chain in LangChain?\", k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2nO0LTA8L43"
      },
      "source": [
        "## 12. Helper Function: Create Sample PDF\n",
        "\n",
        "This creates a sample PDF file for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ul0mL7xz8L44"
      },
      "outputs": [],
      "source": [
        "# def create_sample_pdf(filename=\"sample_document.pdf\"):\n",
        "#     \"\"\"Create a sample PDF for testing.\"\"\"\n",
        "#     from reportlab.lib.pagesizes import letter\n",
        "#     from reportlab.pdfgen import canvas\n",
        "\n",
        "#     c = canvas.Canvas(filename, pagesize=letter)\n",
        "#     c.drawString(100, 750, \"Sample Document for RAG Testing\")\n",
        "#     c.drawString(100, 730, \"\")\n",
        "#     c.drawString(100, 710, \"This is a sample document created for testing the RAG system.\")\n",
        "#     c.drawString(100, 690, \"It contains information about artificial intelligence and machine learning.\")\n",
        "#     c.save()\n",
        "#     print(f\"✓ Created {filename}\")\n",
        "\n",
        "# Uncomment to create a sample PDF\n",
        "# create_sample_pdf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhG0cYNk8L44"
      },
      "source": [
        "## 13. Save and Export Functions\n",
        "\n",
        "Functions to save results for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wXJ24Gj8L44"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "def save_qa_session(questions_and_answers: List[Dict], filename: str = None):\n",
        "    \"\"\"\n",
        "    Save Q&A session to a JSON file.\n",
        "\n",
        "    Args:\n",
        "        questions_and_answers: List of Q&A dictionaries\n",
        "        filename: Output filename (auto-generated if None)\n",
        "    \"\"\"\n",
        "    if filename is None:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"qa_session_{timestamp}.json\"\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(questions_and_answers, f, indent=2)\n",
        "\n",
        "    print(f\"✓ Session saved to {filename}\")\n",
        "\n",
        "# Example usage:\n",
        "# qa_history = []\n",
        "# result = rag.query(\"What is LangChain?\")\n",
        "# qa_history.append({\n",
        "#     \"question\": \"What is LangChain?\",\n",
        "#     \"answer\": result[\"answer\"],\n",
        "#     \"timestamp\": datetime.now().isoformat()\n",
        "# })\n",
        "# save_qa_session(qa_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imM1ujhF8L44"
      },
      "source": [
        "## 15. Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions:\n",
        "\n",
        "1. **Rate Limit Errors**:\n",
        "   - Free tier has 15 RPM (requests per minute)\n",
        "   - Add delays between requests if needed\n",
        "   - Reduce chunk count or batch size\n",
        "\n",
        "2. **API Key Issues**:\n",
        "   - Verify key is correct\n",
        "   - Check key has not expired\n",
        "   - Ensure key is properly set in environment\n",
        "\n",
        "3. **Document Loading Errors**:\n",
        "   - Check file path is correct\n",
        "   - Verify file format is supported\n",
        "   - Ensure file is not corrupted\n",
        "\n",
        "4. **Memory Issues**:\n",
        "   - Reduce chunk size\n",
        "   - Process documents in smaller batches\n",
        "   - Use smaller documents for testing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
